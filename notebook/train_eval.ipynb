{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a6bf3fa-a14a-4b6b-b2c0-62f82edd93dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw, ImageEnhance\n",
    "import albumentations as A\n",
    "import albumentations.pytorch\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import cv2\n",
    "import re\n",
    "import time\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from retinanet import coco_eval\n",
    "from retinanet import csv_eval\n",
    "from retinanet import model\n",
    "# from retinanet import retina\n",
    "from retinanet.dataloader import *\n",
    "from retinanet.anchors import Anchors\n",
    "from retinanet.losses import *\n",
    "from retinanet.scheduler import *\n",
    "from retinanet.parallel import DataParallelModel, DataParallelCriterion\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch.utils.data.sampler import SequentialSampler, RandomSampler\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36c4b695-a4b3-423c-971b-9fe8a72ede57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:4\n",
      "Current cuda device  4\n"
     ]
    }
   ],
   "source": [
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# print ('Available devices ', torch.cuda.device_count())\n",
    "\n",
    "# print ('Current cuda device ', torch.cuda.current_device())\n",
    "# print(torch.cuda.get_device_name(device))\n",
    "\n",
    "# GPU 할당 변경하기\n",
    "GPU_NUM = 4 # 원하는 GPU 번호 입력\n",
    "device = torch.device(f'cuda:{GPU_NUM}' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.set_device(device) # change allocation of current GPU\n",
    "print(device)\n",
    "print ('Current cuda device ', torch.cuda.current_device()) # check\n",
    "device_ids = [4,0,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7261f9e1-e017-4227-a146-770eac16ad05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 8 µs, total: 13 µs\n",
      "Wall time: 27.9 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "PATH_TO_WEIGHTS = '../coco_resnet_50_map_0_335_state_dict.pt'\n",
    "pretrained_retinanet = model.resnet50(num_classes=80, device=device)\n",
    "pretrained_retinanet.load_state_dict(torch.load(PATH_TO_WEIGHTS))\n",
    "\n",
    "\n",
    "retinanet = model.resnet50(num_classes=5, device=device)\n",
    "for param, state in zip(pretrained_retinanet.parameters(), pretrained_retinanet.state_dict()) :\n",
    "    #print(state)\n",
    "    if 'classificationModel' not in state :\n",
    "        retinanet.state_dict()[state] = param\n",
    "    else :\n",
    "        print(state)\n",
    "    \n",
    "for param, state in zip(pretrained_retinanet.fpn.parameters(), pretrained_retinanet.fpn.state_dict()) :\n",
    "    #print(state)\n",
    "    retinanet.fpn.state_dict()[state] = param\n",
    "\n",
    "for param, state in zip(pretrained_retinanet.regressionModel.parameters(), pretrained_retinanet.regressionModel.state_dict()) :\n",
    "    #print(state)\n",
    "    retinanet.regressionModel.state_dict()[state] = param  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2785011-630d-4caf-9554-8a5474c4c2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# retinanet.to(device)\n",
    "retinanet = torch.nn.DataParallel(retinanet, device_ids = [4,0,2,3], output_device=4).to(device)\n",
    "# retinanet = DataParallelModel(retinanet, device_ids = device_ids)\n",
    "retinanet.to(device)\n",
    "# retinanet.cuda()\n",
    "retinanet.module.freeze_bn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1623c623-8e54-410a-acea-3c815718de1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retinanet.load_state_dict(torch.load('best_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b84965f0-f01a-4ee4-8e9d-faba1fda879b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_info = np.load('../data/train.npy', allow_pickle=True, encoding='latin1').item()\n",
    "# train_info\n",
    "\n",
    "batch_size = 32\n",
    "train_ds = PapsDataset(train_info, transforms)\n",
    "\n",
    "train_data_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9772ec28-1372-4ded-bf81-d0c922a16d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = FocalLoss(device)\n",
    "# criterion = DataParallelCriterion(criterion, device_ids = device_ids) \n",
    "criterion = criterion.to(device)\n",
    "# optimizer = optim.Adam(retinanet.parameters(), lr=1e-4)\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, verbose=True)\n",
    "# retinanet.train()\n",
    "retinanet.training = True\n",
    "# retinanet.module.freeze_bn()    \n",
    "# retinanet.freeze_bn()\n",
    "\n",
    "# https://gaussian37.github.io/dl-pytorch-lr_scheduler/\n",
    "optimizer = optim.Adam(retinanet.parameters(), lr = 1e-7)\n",
    "scheduler = CosineAnnealingWarmUpRestarts(optimizer, T_0=20, T_mult=1, eta_max=0.0001,  T_up=5, gamma=0.5)\n",
    "# CosineAnnealingWarmRestarts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d82883f3-febe-4bb3-9292-4b811ab441db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #for i, data in enumerate(tqdm(train_data_loader)) :\n",
    "# EPOCH_NUM = 60\n",
    "# loss_per_epoch = 0.5\n",
    "# for epoch in range(EPOCH_NUM) :\n",
    "#     epoch_loss = []\n",
    "#     total_loss = 0\n",
    "#     tk0 = tqdm(train_data_loader, total=len(train_data_loader), leave=False)\n",
    "#     EPOCH_LEARING_RATE = optimizer.param_groups[0][\"lr\"]\n",
    "#     print(\"*****{}th epoch, learning rate {}\".format(epoch, EPOCH_LEARING_RATE))\n",
    "    \n",
    "#     for step, data in enumerate(tk0) :\n",
    "#         images, _, paths, targets = data\n",
    "# #         print(targets)\n",
    "#         batch_size = len(images)\n",
    "\n",
    "#     #     images = list(image.to(device) for image in images)\n",
    "#         c, h, w = images[0].shape\n",
    "#         images = torch.cat(images).view(-1, c, h, w).to(device)\n",
    "# #         targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "#         targets = [ t.to(device) for t in targets]\n",
    "\n",
    "# #         classification_loss, regression_loss = retinanet([images, targets])\n",
    "#         outputs = retinanet([images, targets])\n",
    "#         classification, regression, anchors, annotations = (outputs)\n",
    "#         classification_loss, regression_loss = criterion(classification, regression, anchors, annotations)\n",
    "\n",
    "# #         output = retinanet(images)\n",
    "# #         features, regression, classification = output\n",
    "# #         classification_loss, regression_loss = criterion(classification, regression, modified_anchors, targets)    \n",
    "#         classification_loss = classification_loss.mean()\n",
    "#         regression_loss = regression_loss.mean()\n",
    "#         loss = classification_loss + regression_loss \n",
    "#         total_loss += loss.item()\n",
    "        \n",
    "#         epoch_loss.append((loss.item()))\n",
    "#         tk0.set_postfix(lr=optimizer.param_groups[0][\"lr\"], batch_loss=loss.item(), cls_loss=classification_loss.item(), \n",
    "#                         reg_loss=regression_loss.item(), avg_loss=total_loss/(step+1))\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         torch.nn.utils.clip_grad_norm_(retinanet.parameters(), 0.1)\n",
    "#         optimizer.step()   \n",
    "\n",
    "#     print('{}th epochs loss is {}'.format(epoch, np.mean(epoch_loss)))\n",
    "#     if loss_per_epoch > np.mean(epoch_loss):\n",
    "#         print('best model is saved')\n",
    "#         torch.save(retinanet.state_dict(), 'best_model.pt')\n",
    "#         loss_per_epoch = np.mean(epoch_loss)\n",
    "# #     scheduler.step(np.mean(epoch_loss))\n",
    "#     scheduler.step()\n",
    "\n",
    "# torch.save(retinanet.state_dict(), '../trained_models/model.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2edd8d36-13d3-4e68-8b7e-aeae46c43bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_info = np.load('../data/test.npy', allow_pickle=True, encoding='latin1').item()\n",
    "test_ds = PapsDataset(test_info,val_transforms)\n",
    "\n",
    "test_data_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41860d05-80b5-4117-b091-02a4d594b619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.19s)\n",
      "creating index...\n",
      "index created!\n",
      "3739\n"
     ]
    }
   ],
   "source": [
    "#dataset_val = COCO('../data/coco/val.json')\n",
    "dataset_val = CocoDataset('../data/', set_name='val',\n",
    "                                  transform=val_transforms, isPapsmear=True)\n",
    "print(len(dataset_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6fdd46bb-f4b5-4ba8-855c-28bf6279b872",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_val[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cfa67a9d-7314-4a49-bc7d-66a54a8747d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0b139dfbd5145f792003dfb055e7224",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3744 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1800, 4000, 3)\n",
      "anno [[1978.  867. 2068.  996.    5.]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Your 'label_fields' are not valid - them must have same names as params in dict",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-85cda1ceeb0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#for step, data in enumerate(tk1) :\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mscale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'scale'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/retinanet/pytorch-retinanet/retinanet/dataloader.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'annot'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mannot\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch_retina/lib/python3.7/site-packages/albumentations/core/composition.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, force_apply, *args, **data)\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0mneed_to_run\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforce_apply\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_data_valid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0mtransforms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mneed_to_run\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_always_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0mdual_start_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_end\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessors\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch_retina/lib/python3.7/site-packages/albumentations/augmentations/bbox_utils.py\u001b[0m in \u001b[0;36mensure_data_valid\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_fields\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_fields\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Your 'label_fields' are not valid - them must have same names as params in dict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Your 'label_fields' are not valid - them must have same names as params in dict"
     ]
    }
   ],
   "source": [
    "from pycocotools.cocoeval import COCOeval\n",
    "from pycocotools.coco import COCO\n",
    "import json\n",
    "import torch\n",
    "\n",
    "retinanet.eval()\n",
    "# retinanet.training = True\n",
    "tk1 = tqdm(test_data_loader, total=len(test_data_loader),leave=False)\n",
    "\n",
    "# start collecting results\n",
    "results = []\n",
    "image_ids = []\n",
    "threshold = 0.05\n",
    "with torch.no_grad():\n",
    "#for step, data in enumerate(tk1) :\n",
    "    for index in range(len(dataset_val)):\n",
    "        data = dataset_val[index]\n",
    "        scale = data['scale']\n",
    "        print(data)\n",
    "\n",
    "        images, _, path, targets = data\n",
    "#         print(path)\n",
    "        batch_size = len(images)\n",
    "        c, h, w = images[0].shape\n",
    "        images = torch.cat(images).view(-1, c, h, w).to(device)\n",
    "#         print(images.shape)\n",
    "        targets = [ t.to(device) for t in targets]   \n",
    "        \n",
    "        scores, labels, boxes = retinanet(images)\n",
    "        if boxes.shape[0] > 0:\n",
    "            # change to (x, y, w, h) (MS COCO standard)\n",
    "            boxes[:, 2] -= boxes[:, 0]\n",
    "            boxes[:, 3] -= boxes[:, 1]     \n",
    "            \n",
    "            # compute predicted labels and scores\n",
    "            #for box, score, label in zip(boxes[0], scores[0], labels[0]):\n",
    "            for box_id in range(boxes.shape[0]):\n",
    "                score = float(scores[box_id])\n",
    "                label = int(labels[box_id])\n",
    "                box = boxes[box_id, :]\n",
    "\n",
    "                # scores are sorted, so we can break\n",
    "                if score < threshold:\n",
    "                    break\n",
    "\n",
    "                # append detection for each positively labeled class\n",
    "                image_result = {\n",
    "#                     'image_id'    : dataset.image_ids[index],\n",
    "#                     'category_id' : dataset.label_to_coco_label(label),\n",
    "                    'image_id'    : path[0],\n",
    "                    'category_id' : label,\n",
    "                    'score'       : float(score),\n",
    "                    'bbox'        : box.tolist(),\n",
    "                }\n",
    "\n",
    "                # append detection to results\n",
    "                results.append(image_result)    \n",
    "                \n",
    "        # append image to list of processed images\n",
    "        image_ids.append(path)\n",
    "        if step > 100 :\n",
    "            break\n",
    "        \n",
    "if not len(results):\n",
    "    print(\"no results at all\")\n",
    "\n",
    "# write output\n",
    "json.dump(results, open('../data/papsmear_bbox_results.json', 'w'), indent=4)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4c000c-9770-486b-85ea-87df6fa6e27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8edd49-a234-441d-9ec3-a285f2561bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # df = pd.read_csv('df.csv')\n",
    "# test_info = np.load('../data/test.npy', allow_pickle=True, encoding='latin1').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4e5cfa-5338-43bf-84be-e8e97d6cf2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paps_GT[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d570f4eb-caee-42a9-b7ab-179d888d6560",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd25d99-0595-46f7-899f-a568646ea83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(test_info['/home/Dataset/Papsmear/original/SS2/03(200908-10-normal)/20200908_093508(0).jpg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bdc71a-9e27-4109-a761-892bc1b2e6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_info\n",
    "paps_GT = []\n",
    "for path in test_info.keys():\n",
    "    label = test_info[path]\n",
    "    for i in range(len(label)) :\n",
    "        paps_gt = {\n",
    "                'image_id'    : path,\n",
    "                'category_id' : label[i][5],\n",
    "                'score'       : 1,\n",
    "                'bbox'        : box.tolist(),\n",
    "            }\n",
    "        paps_GT.append(paps_gt) \n",
    "        \n",
    "json.dump(paps_GT, open('../data/papsmear_GT.json', 'w'), indent=4)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ea54e8-7136-484c-b6b8-0502245cea6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "coco = COCO('../data/coco/val.json')\n",
    "print(type(coco))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fae1698-1a57-479a-b6b7-480f4ea00eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coco.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e3e474-61c5-47e1-9a04-a16c71fe6a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paps_pred = coco.loadRes('../data/papsmear_bbox_results.json')\n",
    "paps_gt = coco.loadRes('../data/papsmear_bbox_results.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892a33a7-5fb7-45d7-9d69-d679d177e0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../data/papsmear_bbox_results.json', 'r') as f:\n",
    "#     paps_pred = json.load(f)\n",
    "# with open('../data/papsmear_GT.json', 'r') as f:\n",
    "#     paps_gt = json.load(f)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443163ac-4413-4c63-9290-1677e9b70e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run COCO evaluation\n",
    "coco_eval = COCOeval(paps_gt, paps_pred, 'bbox')\n",
    "coco_eval.params.imgIds = image_ids\n",
    "# coco_eval.params.catIds = [1]\n",
    "coco_eval.evaluate()\n",
    "coco_eval.accumulate()\n",
    "coco_eval.summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d22b90-7b4a-44bf-a178-f365e23cf025",
   "metadata": {},
   "outputs": [],
   "source": [
    "paps_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28815e80-5552-42f0-9a14-7b32801f0b78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_retina",
   "language": "python",
   "name": "torch_retina"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
