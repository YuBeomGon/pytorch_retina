{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eaa2af7c-0c0b-4972-9cfb-12d750446ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw, ImageEnhance\n",
    "import albumentations as A\n",
    "import albumentations.pytorch\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import cv2\n",
    "import re\n",
    "import time\n",
    "\n",
    "import sys\n",
    "sys.path.append('../utils')\n",
    "sys.path.append('../')\n",
    "from visualize import *\n",
    "from xml_parser import *\n",
    "from transformations import *\n",
    "from retinanet import coco_eval\n",
    "from retinanet import csv_eval\n",
    "from retinanet import model\n",
    "# from retinanet import retina\n",
    "from retinanet.retina import *\n",
    "from retinanet.augment import *\n",
    "from retinanet.dataloader import *\n",
    "from retinanet.anchors import Anchors\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch.utils.data.sampler import SequentialSampler, RandomSampler\n",
    "from torch.optim import Adam, lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0110275c-0373-4c6b-92bf-204d3e2b8c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Anchors(nn.Module):\n",
    "    def __init__(self, pyramid_levels=None, strides=None, sizes=None, ratios=None, scales=None):\n",
    "        super(Anchors, self).__init__()\n",
    "\n",
    "        self.pyramid_levels = pyramid_levels\n",
    "        self.strides = strides\n",
    "        self.sizes = sizes\n",
    "        self.ratios = ratios\n",
    "        self.scales = scales\n",
    "\n",
    "        if pyramid_levels is None:\n",
    "            self.pyramid_levels = [3, 4, 5, 6, 7]\n",
    "        if strides is None:\n",
    "            self.strides = [2 ** x for x in self.pyramid_levels]\n",
    "        if sizes is None:\n",
    "            self.sizes = [2 ** (x + 2) for x in self.pyramid_levels]\n",
    "        if ratios is None:\n",
    "            self.ratios = np.array([0.5, 1, 2])\n",
    "        if scales is None:\n",
    "            self.scales = np.array([2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)])\n",
    "        print(self.pyramid_levels )\n",
    "\n",
    "    def forward(self, image):\n",
    "        \n",
    "        image_shape = image.shape[2:]\n",
    "        image_shape = np.array(image_shape)\n",
    "        image_shapes = [(image_shape + 2 ** x - 1) // (2 ** x) for x in self.pyramid_levels]\n",
    "\n",
    "        # compute anchors over all pyramid levels\n",
    "        all_anchors = np.zeros((0, 4)).astype(np.float32)\n",
    "\n",
    "        for idx, p in enumerate(self.pyramid_levels):\n",
    "            anchors         = generate_anchors(base_size=self.sizes[idx], ratios=self.ratios, scales=self.scales)\n",
    "            shifted_anchors = shift(image_shapes[idx], self.strides[idx], anchors)\n",
    "            all_anchors     = np.append(all_anchors, shifted_anchors, axis=0)\n",
    "\n",
    "        all_anchors = np.expand_dims(all_anchors, axis=0)\n",
    "#         print((all_anchors))\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            return torch.from_numpy(all_anchors.astype(np.float32)).cuda()\n",
    "        else:\n",
    "            return torch.from_numpy(all_anchors.astype(np.float32))\n",
    "\n",
    "def generate_anchors(base_size=16, ratios=None, scales=None):\n",
    "    \"\"\"\n",
    "    Generate anchor (reference) windows by enumerating aspect ratios X\n",
    "    scales w.r.t. a reference window.\n",
    "    \"\"\"\n",
    "\n",
    "    if ratios is None:\n",
    "        ratios = np.array([0.5, 1, 2])\n",
    "\n",
    "    if scales is None:\n",
    "        scales = np.array([2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)])\n",
    "\n",
    "    num_anchors = len(ratios) * len(scales)\n",
    "\n",
    "    # initialize output anchors\n",
    "    anchors = np.zeros((num_anchors, 4))\n",
    "\n",
    "    # scale base_size\n",
    "    anchors[:, 2:] = base_size * np.tile(scales, (2, len(ratios))).T\n",
    "\n",
    "    # compute areas of anchors\n",
    "    areas = anchors[:, 2] * anchors[:, 3]\n",
    "\n",
    "    # correct for ratios\n",
    "    anchors[:, 2] = np.sqrt(areas / np.repeat(ratios, len(scales)))\n",
    "    anchors[:, 3] = anchors[:, 2] * np.repeat(ratios, len(scales))\n",
    "\n",
    "    # transform from (x_ctr, y_ctr, w, h) -> (x1, y1, x2, y2)\n",
    "    anchors[:, 0::2] -= np.tile(anchors[:, 2] * 0.5, (2, 1)).T\n",
    "    anchors[:, 1::2] -= np.tile(anchors[:, 3] * 0.5, (2, 1)).T\n",
    "\n",
    "    return anchors\n",
    "\n",
    "def compute_shape(image_shape, pyramid_levels):\n",
    "    \"\"\"Compute shapes based on pyramid levels.\n",
    "\n",
    "    :param image_shape:\n",
    "    :param pyramid_levels:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    image_shape = np.array(image_shape[:2])\n",
    "    image_shapes = [(image_shape + 2 ** x - 1) // (2 ** x) for x in pyramid_levels]\n",
    "    return image_shapes\n",
    "\n",
    "\n",
    "def anchors_for_shape(\n",
    "    image_shape,\n",
    "    pyramid_levels=None,\n",
    "    ratios=None,\n",
    "    scales=None,\n",
    "    strides=None,\n",
    "    sizes=None,\n",
    "    shapes_callback=None,\n",
    "):\n",
    "\n",
    "    image_shapes = compute_shape(image_shape, pyramid_levels)\n",
    "\n",
    "    # compute anchors over all pyramid levels\n",
    "    all_anchors = np.zeros((0, 4))\n",
    "    for idx, p in enumerate(pyramid_levels):\n",
    "        anchors         = generate_anchors(base_size=sizes[idx], ratios=ratios, scales=scales)\n",
    "        shifted_anchors = shift(image_shapes[idx], strides[idx], anchors)\n",
    "        all_anchors     = np.append(all_anchors, shifted_anchors, axis=0)\n",
    "\n",
    "    return all_anchors\n",
    "\n",
    "\n",
    "def shift(shape, stride, anchors):\n",
    "    shift_x = (np.arange(0, shape[1]) + 0.5) * stride\n",
    "    shift_y = (np.arange(0, shape[0]) + 0.5) * stride\n",
    "\n",
    "    shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n",
    "\n",
    "    shifts = np.vstack((\n",
    "        shift_x.ravel(), shift_y.ravel(),\n",
    "        shift_x.ravel(), shift_y.ravel()\n",
    "    )).transpose()\n",
    "\n",
    "    # add A anchors (1, A, 4) to\n",
    "    # cell K shifts (K, 1, 4) to get\n",
    "    # shift anchors (K, A, 4)\n",
    "    # reshape to (K*A, 4) shifted anchors\n",
    "    A = anchors.shape[0]\n",
    "    K = shifts.shape[0]\n",
    "    all_anchors = (anchors.reshape((1, A, 4)) + shifts.reshape((1, K, 4)).transpose((1, 0, 2)))\n",
    "    all_anchors = all_anchors.reshape((K * A, 4))\n",
    "\n",
    "    return all_anchors        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "00acb811-988a-4939-a225-6595c62db313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 4, 5, 6, 7]\n"
     ]
    }
   ],
   "source": [
    "ratios=np.array([1])\n",
    "scales=np.array([1])\n",
    "# anchors = Anchors(pyramid_levels=[3, 4, 5], strides=None, sizes=None, ratios=ratios, scales=scales)\n",
    "anchors = Anchors(pyramid_levels=None, strides=None, sizes=None, ratios=None, scales=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b9847bd7-2bef-4d52-a833-afd453585854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 640, 640])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = torch.randn([2, 3, 640, 640])\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "db6868a0-59d3-4c6a-8c63-1bb44c6e1526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ -18.627417     -7.3137085    26.627417     15.3137085 ]\n",
      "  [ -24.50875898  -10.25437949   32.50875898   18.25437949]\n",
      "  [ -31.91878555  -13.95939277   39.91878555   21.95939277]\n",
      "  ...\n",
      "  [ 394.98066402  213.96132803  757.01933598  938.03867197]\n",
      "  [ 347.92992816  119.85985631  804.07007184 1032.14014369]\n",
      "  [ 288.64971563    1.29943127  863.35028437 1150.70056873]]]\n"
     ]
    }
   ],
   "source": [
    "ans = anchors(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "14d8f054-283a-4e14-bb47-7538e3119336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-12., -12.,  20.,  20.],\n",
       "        [ -4., -12.,  28.,  20.],\n",
       "        [  4., -12.,  36.,  20.],\n",
       "        [ 12., -12.,  44.,  20.],\n",
       "        [ 20., -12.,  52.,  20.],\n",
       "        [ 28., -12.,  60.,  20.],\n",
       "        [ 36., -12.,  68.,  20.],\n",
       "        [ 44., -12.,  76.,  20.],\n",
       "        [ 52., -12.,  84.,  20.],\n",
       "        [ 60., -12.,  92.,  20.]], device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans[0][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "166d5c9f-8507-4895-92fe-ebac59eeacee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5, 1. , 2. ])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratios = np.array([0.5, 1, 2])\n",
    "ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2331fa44-b0a6-47be-bfd1-71d8e90e8ddd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 1.25992105, 1.58740105])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scales = np.array([2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)])\n",
    "scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fa3ac184-c6d3-4cf1-bdd3-9ae8e274fe02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_anchors = len(ratios) * len(scales)\n",
    "num_anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ff16d2c1-400c-4a95-b603-7fc9e3225285",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize output anchors\n",
    "anchors = np.zeros((num_anchors, 4))\n",
    "anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0484bf1b-7ce0-4ec8-b935-71312b51e228",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        , 16.        , 16.        ],\n",
       "       [ 0.        ,  0.        , 20.1587368 , 20.1587368 ],\n",
       "       [ 0.        ,  0.        , 25.39841683, 25.39841683],\n",
       "       [ 0.        ,  0.        , 16.        , 16.        ],\n",
       "       [ 0.        ,  0.        , 20.1587368 , 20.1587368 ],\n",
       "       [ 0.        ,  0.        , 25.39841683, 25.39841683],\n",
       "       [ 0.        ,  0.        , 16.        , 16.        ],\n",
       "       [ 0.        ,  0.        , 20.1587368 , 20.1587368 ],\n",
       "       [ 0.        ,  0.        , 25.39841683, 25.39841683]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scale base_size\n",
    "base_size = 16\n",
    "anchors[:, 2:] = base_size * np.tile(scales, (2, len(ratios))).T\n",
    "anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b8e5d46d-a38f-40a0-be16-d166ef61ea24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([256.        , 406.3746693 , 645.07957755, 256.        ,\n",
       "       406.3746693 , 645.07957755, 256.        , 406.3746693 ,\n",
       "       645.07957755])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute areas of anchors\n",
    "areas = anchors[:, 2] * anchors[:, 3]\n",
    "areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7ffae915-f97b-4cb0-88f0-c56176ac22bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        , 22.627417  , 11.3137085 ],\n",
       "       [ 0.        ,  0.        , 28.50875898, 14.25437949],\n",
       "       [ 0.        ,  0.        , 35.91878555, 17.95939277],\n",
       "       [ 0.        ,  0.        , 16.        , 16.        ],\n",
       "       [ 0.        ,  0.        , 20.1587368 , 20.1587368 ],\n",
       "       [ 0.        ,  0.        , 25.39841683, 25.39841683],\n",
       "       [ 0.        ,  0.        , 11.3137085 , 22.627417  ],\n",
       "       [ 0.        ,  0.        , 14.25437949, 28.50875898],\n",
       "       [ 0.        ,  0.        , 17.95939277, 35.91878555]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# correct for ratios\n",
    "anchors[:, 2] = np.sqrt(areas / np.repeat(ratios, len(scales)))\n",
    "anchors[:, 3] = anchors[:, 2] * np.repeat(ratios, len(scales))\n",
    "anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a790594c-4724-4bc9-9882-6086d2bc1bdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-11.3137085 ,  -5.65685425,  11.3137085 ,   5.65685425],\n",
       "       [-14.25437949,  -7.12718975,  14.25437949,   7.12718975],\n",
       "       [-17.95939277,  -8.97969639,  17.95939277,   8.97969639],\n",
       "       [ -8.        ,  -8.        ,   8.        ,   8.        ],\n",
       "       [-10.0793684 , -10.0793684 ,  10.0793684 ,  10.0793684 ],\n",
       "       [-12.69920842, -12.69920842,  12.69920842,  12.69920842],\n",
       "       [ -5.65685425, -11.3137085 ,   5.65685425,  11.3137085 ],\n",
       "       [ -7.12718975, -14.25437949,   7.12718975,  14.25437949],\n",
       "       [ -8.97969639, -17.95939277,   8.97969639,  17.95939277]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# transform from (x_ctr, y_ctr, w, h) -> (x1, y1, x2, y2)\n",
    "anchors[:, 0::2] -= np.tile(anchors[:, 2] * 0.5, (2, 1)).T\n",
    "anchors[:, 1::2] -= np.tile(anchors[:, 3] * 0.5, (2, 1)).T\n",
    "anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ff60ac-be9f-46e8-be77-0638bb57d67f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7ad838-0def-4e43-b9ec-dee5133f4e20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afdfc3a-b6e4-4b6c-b81d-fd99e3022d22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2283089-6da3-4d8f-a823-aae05bd423ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize output anchors\n",
    "\n",
    "anchors = np.zeros((num_anchors, 4))\n",
    "\n",
    "# scale base_size\n",
    "anchors[:, 2:] = base_size * np.tile(scales, (2, len(ratios))).T\n",
    "\n",
    "# compute areas of anchors\n",
    "areas = anchors[:, 2] * anchors[:, 3]\n",
    "\n",
    "# correct for ratios\n",
    "anchors[:, 2] = np.sqrt(areas / np.repeat(ratios, len(scales)))\n",
    "anchors[:, 3] = anchors[:, 2] * np.repeat(ratios, len(scales))\n",
    "\n",
    "# transform from (x_ctr, y_ctr, w, h) -> (x1, y1, x2, y2)\n",
    "anchors[:, 0::2] -= np.tile(anchors[:, 2] * 0.5, (2, 1)).T\n",
    "anchors[:, 1::2] -= np.tile(anchors[:, 3] * 0.5, (2, 1)).T"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_retina",
   "language": "python",
   "name": "torch_retina"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
