{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0def42c-30fc-4af5-a05d-2c2dd43ad867",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "# import sys\n",
    "# sys.path.append('/opt/conda/lib/python3.7/site-packages')\n",
    "\n",
    "from torchvision.models.detection import RetinaNet\n",
    "from torchvision.models.detection.anchor_utils import AnchorGenerator\n",
    "# from torchvision.transforms import GeneralizedRCNNTransform\n",
    "# import albumentations as A\n",
    "\n",
    "import math\n",
    "from collections import OrderedDict\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9712a451-e283-475b-854f-0ddb2939ae0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ImageTransform(nn.Module) :\n",
    "#     def __init__(self, image_mean, image_std, size_divisible=32, fixed_size=None):\n",
    "#         #super(ImageTransform, self).__init__()  \n",
    "#         self.image_mean = image_mean\n",
    "#         self.image_std = image_std\n",
    "#         self.size_divisible = size_divisible\n",
    "#         self.fixed_size = fixed_size      \n",
    "        \n",
    "#     def forward(self,\n",
    "#                 images,       # type: List[Tensor]\n",
    "#                 targets=None  # type: Optional[List[Dict[str, Tensor]]]\n",
    "#                 ):\n",
    "#         # type: (...) -> Tuple[ImageList, Optional[List[Dict[str, Tensor]]]]\n",
    "#         images = [img for img in images]\n",
    "#         if targets is not None:\n",
    "#             # make a copy of targets to avoid modifying it in-place\n",
    "#             # once torchscript supports dict comprehension\n",
    "#             # this can be simplified as follows\n",
    "#             # targets = [{k: v for k,v in t.items()} for t in targets]\n",
    "#             targets_copy: List[Dict[str, Tensor]] = []\n",
    "#             for t in targets:\n",
    "#                 data: Dict[str, Tensor] = {}\n",
    "#                 for k, v in t.items():\n",
    "#                     data[k] = v\n",
    "#                 targets_copy.append(data)\n",
    "#             targets = targets_copy\n",
    "#         for i in range(len(images)):\n",
    "#             image = images[i]\n",
    "#             target_index = targets[i] if targets is not None else None\n",
    "\n",
    "#             if image.dim() != 3:\n",
    "#                 raise ValueError(\"images is expected to be a list of 3d tensors \"\n",
    "#                                  \"of shape [C, H, W], got {}\".format(image.shape))\n",
    "#             image = self.normalize(image)\n",
    "# #            image, target_index = self.resize(image, target_index)\n",
    "#             images[i] = image\n",
    "#             if targets is not None and target_index is not None:\n",
    "#                 targets[i] = target_index\n",
    "\n",
    "#         image_sizes = [img.shape[-2:] for img in images]\n",
    "#         images = self.batch_images(images, size_divisible=self.size_divisible)\n",
    "#         image_sizes_list: List[Tuple[int, int]] = []\n",
    "#         for image_size in image_sizes:\n",
    "#             assert len(image_size) == 2\n",
    "#             image_sizes_list.append((image_size[0], image_size[1]))\n",
    "\n",
    "#         image_list = ImageList(images, image_sizes_list)\n",
    "#         return image_list, targets        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78dab006-2f3f-4515-959e-a1439742d4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transforms():\n",
    "    return A.Compose(\n",
    "        [\n",
    "            A.OneOf(\n",
    "            [\n",
    "                A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit= 0.2, val_shift_limit=0.2, p=0.9),      \n",
    "                A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.9)\n",
    "            ],\n",
    "            p=0.9),         \n",
    "            #A.ToGray(p=0.01),         \n",
    "            A.HorizontalFlip(p=0.5),         \n",
    "            A.VerticalFlip(p=0.5),         \n",
    "            A.Resize(height=512, width=512, p=1),      \n",
    "            A.Normalize(max_pixel_value=1),\n",
    "            #A.Cutout(num_holes=8, max_h_size=32, max_w_size=32, fill_value=0, p=0.5),\n",
    "            ToTensorV2(p=1.0)\n",
    "        ], \n",
    "        p=1.0,         \n",
    "        bbox_params=A.BboxParams(format='coco',min_area=0, min_visibility=0.99,label_fields=['labels'])\n",
    "        )\n",
    "\n",
    "def get_valid_transforms():\n",
    "    return A.Compose([A.Resize(height=512, width=512, p=1.0),\n",
    "                      A.Normalize(max_pixel_value=1),\n",
    "                      ToTensorV2(p=1.0),\n",
    "                      ], \n",
    "                      p=1.0, \n",
    "                      bbox_params=A.BboxParams(format='coco',min_area=0, min_visibility=0,label_fields=['labels'])\n",
    "                      )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd9685a0-91c6-4445-93b9-2499392da750",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "class BlockAfterFPN(nn.Module) :\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BlockAfterFPN, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.conv3 = conv3x3(planes, inplanes)\n",
    "        self.bn3 = nn.BatchNorm2d(inplanes)\n",
    "        self.relu3 = nn.ReLU(inplace=True)        \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu1(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu2(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        out = self.relu3(out)\n",
    "\n",
    "        out += residual\n",
    "#         out = self.relu(out)\n",
    "        return out    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0d55eb1-085a-4be5-b5f8-c42fc26c2fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class mobilenetv2_retinanet(RetinaNet) :\n",
    "    def __init__(self, backbone, num_classes,\n",
    "                 anchor_generator=None):\n",
    "        \n",
    "        super().__init__(backbone, num_classes,\n",
    "                 anchor_generator=anchor_generator)\n",
    "    \n",
    "#         super().__init__(backbone, num_classes=num_classes)\n",
    "#         if image_mean is None:\n",
    "#             image_mean = [0.485, 0.456, 0.406]\n",
    "#         if image_std is None:\n",
    "#             image_std = [0.229, 0.224, 0.225]\n",
    "#         self.transform = ImageTransform(image_mean, image_std)\n",
    "        self.BlockAfterFPN = BlockAfterFPN(inplanes=1280, planes=int(1280/4))\n",
    "        \n",
    "    def forward(self, images, targets=None):\n",
    "        # type: (List[Tensor], Optional[List[Dict[str, Tensor]]]) -> Tuple[Dict[str, Tensor], List[Dict[str, Tensor]]]\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images (list[Tensor]): images to be processed\n",
    "            targets (list[Dict[Tensor]]): ground-truth boxes present in the image (optional)\n",
    "\n",
    "        Returns:\n",
    "            result (list[BoxList] or dict[Tensor]): the output from the model.\n",
    "                During training, it returns a dict[Tensor] which contains the losses.\n",
    "                During testing, it returns list[BoxList] contains additional fields\n",
    "                like `scores`, `labels` and `mask` (for Mask R-CNN models).\n",
    "\n",
    "        \"\"\"\n",
    "        if self.training and targets is None:\n",
    "            raise ValueError(\"In training mode, targets should be passed\")\n",
    "\n",
    "        if self.training:\n",
    "            assert targets is not None\n",
    "            for target in targets:\n",
    "                boxes = target[\"boxes\"]\n",
    "                if isinstance(boxes, torch.Tensor):\n",
    "                    if len(boxes.shape) != 2 or boxes.shape[-1] != 4:\n",
    "                        raise ValueError(\"Expected target boxes to be a tensor\"\n",
    "                                         \"of shape [N, 4], got {:}.\".format(\n",
    "                                             boxes.shape))\n",
    "                else:\n",
    "                    raise ValueError(\"Expected target boxes to be of type \"\n",
    "                                     \"Tensor, got {:}.\".format(type(boxes)))\n",
    "\n",
    "        # get the original image sizes\n",
    "        original_image_sizes: List[Tuple[int, int]] = []\n",
    "        for img in images:\n",
    "            val = img.shape[-2:]\n",
    "            assert len(val) == 2\n",
    "            original_image_sizes.append((val[0], val[1]))\n",
    "\n",
    "        # transform the input\n",
    "        print(type(images))\n",
    "#         images, targets = self.transform(images, targets)\n",
    "        print(len(images))\n",
    "        features = torch.cat([torch.unsqueeze(torch.tensor(image), 0) for image in images], dim=0)\n",
    "#        image_sizes_list = []\n",
    "#        for image in images :\n",
    "#             print(image.shape)\n",
    "#            image_sizes_list.append([image.shape[1], image.shape[2]])\n",
    "#        images = ImageList(images, image_sizes_list)\n",
    "        \n",
    "        \n",
    "#         print(images.size())\n",
    "\n",
    "        # Check for degenerate boxes\n",
    "        # TODO: Move this to a function\n",
    "        if targets is not None:\n",
    "            for target_idx, target in enumerate(targets):\n",
    "                boxes = target[\"boxes\"]\n",
    "                degenerate_boxes = boxes[:, 2:] <= boxes[:, :2]\n",
    "                if degenerate_boxes.any():\n",
    "                    # print the first degenerate box\n",
    "                    bb_idx = torch.where(degenerate_boxes.any(dim=1))[0][0]\n",
    "                    degen_bb: List[float] = boxes[bb_idx].tolist()\n",
    "                    raise ValueError(\"All bounding boxes should have positive height and width.\"\n",
    "                                     \" Found invalid box {} for target at index {}.\"\n",
    "                                     .format(degen_bb, target_idx))\n",
    "\n",
    "        # get the features from the backbone\n",
    "        features = self.backbone(features)\n",
    "        print('feature size', features.size())\n",
    "#         print(type(images.tensors))\n",
    "#         print((images.tensors.size()))\n",
    "        if isinstance(features, torch.Tensor):\n",
    "            features = OrderedDict([('0', features)])\n",
    "\n",
    "        # TODO: Do we want a list or a dict?\n",
    "        features = list(features.values())\n",
    "\n",
    "        # compute the retinanet heads outputs using the features\n",
    "        head_outputs = self.head(features)\n",
    "\n",
    "        # create the set of anchors\n",
    "        anchors = self.anchor_generator(images, features)\n",
    "        print(anchors)\n",
    "\n",
    "        losses = {}\n",
    "        detections: List[Dict[str, Tensor]] = []\n",
    "        if self.training:\n",
    "            assert targets is not None\n",
    "\n",
    "            # compute the losses\n",
    "            losses = self.compute_loss(targets, head_outputs, anchors)\n",
    "        else:\n",
    "            # recover level sizes\n",
    "            num_anchors_per_level = [x.size(2) * x.size(3) for x in features]\n",
    "            HW = 0\n",
    "            for v in num_anchors_per_level:\n",
    "                HW += v\n",
    "            HWA = head_outputs['cls_logits'].size(1)\n",
    "            A = HWA // HW\n",
    "            num_anchors_per_level = [hw * A for hw in num_anchors_per_level]\n",
    "\n",
    "            # split outputs per level\n",
    "            split_head_outputs: Dict[str, List[Tensor]] = {}\n",
    "            for k in head_outputs:\n",
    "                split_head_outputs[k] = list(head_outputs[k].split(num_anchors_per_level, dim=1))\n",
    "            split_anchors = [list(a.split(num_anchors_per_level)) for a in anchors]\n",
    "\n",
    "            # compute the detections\n",
    "            detections = self.postprocess_detections(split_head_outputs, split_anchors, images.image_sizes)\n",
    "            detections = self.transform.postprocess(detections, images.image_sizes, original_image_sizes)\n",
    "\n",
    "        if torch.jit.is_scripting():\n",
    "            if not self._has_warned:\n",
    "                warnings.warn(\"RetinaNet always returns a (Losses, Detections) tuple in scripting\")\n",
    "                self._has_warned = True\n",
    "            return losses, detections\n",
    "        return self.eager_outputs(losses, detections)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "296f9c27-4736-4f4e-b613-6221ec539e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/beomgon/anaconda3/envs/torch_retina/lib/python3.7/site-packages/ipykernel_launcher.py:57: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature size torch.Size([2, 1280, 32, 32])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'tensors'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-98e7cc12ccc4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1012\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1012\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1012\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1012\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/torch_retina/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-66762c6dca56>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;31m# create the set of anchors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0manchors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manchor_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch_retina/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch_retina/lib/python3.7/site-packages/torchvision/models/detection/anchor_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, image_list, feature_maps)\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_list\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mImageList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_maps\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0mgrid_sizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfeature_map\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeature_maps\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0mimage_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_maps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_maps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         strides = [[torch.tensor(image_size[0] // g[0], dtype=torch.int64, device=device),\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'tensors'"
     ]
    }
   ],
   "source": [
    "backbone = torchvision.models.mobilenet_v2(pretrained=True).features\n",
    "# RetinaNet needs to know the number of\n",
    "# output channels in a backbone. For mobilenet_v2, it's 1280\n",
    "# so we need to add it here\n",
    "backbone.out_channels = 1280\n",
    "\n",
    "# let's make the network generate 5 x 3 anchors per spatial\n",
    "# location, with 5 different sizes and 3 different aspect\n",
    "# ratios. We have a Tuple[Tuple[int]] because each feature\n",
    "# map could potentially have different sizes and\n",
    "# aspect ratios\n",
    "anchor_generator = AnchorGenerator(\n",
    "    sizes=((32, 64, 128, 256, 512),),\n",
    "    aspect_ratios=((0.5, 1.0, 2.0),)\n",
    ")\n",
    "\n",
    "# put the pieces together inside a RetinaNet model\n",
    "model = mobilenetv2_retinanet(backbone,\n",
    "                  num_classes=2,\n",
    "                  anchor_generator=anchor_generator)\n",
    "model.eval()\n",
    "x = [torch.rand(3, 1012, 1012), torch.rand(3, 1012, 1012)]\n",
    "predictions = model(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968e3118-734a-4bb7-958e-8a764637432e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f200f437-4052-4010-ae3a-f353dca62da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [torch.rand(3, 1012, 1012), torch.rand(3, 1012, 1012)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fa019a-68a5-40eb-aa63-3ffdb5c6b4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e99dec-c178-49f6-a429-426291572c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d792f147-f1bf-44ec-a330-5c1428156d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.anchor_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5f2ad9-b0cc-41e2-a934-942c90f6ca9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_retina",
   "language": "python",
   "name": "torch_retina"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
